{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fb0138",
   "metadata": {},
   "source": [
    "# **NLP Medical Question Filtering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1b9be",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868bf1b",
   "metadata": {},
   "source": [
    "### Model Training Pipeline Elements\n",
    "\n",
    "**1. Source Dataset Prep**\n",
    "- Medical Questions Datasets Load & Prepare\n",
    "\n",
    "**2. Perturbation Generation & Sentence Embedding**\n",
    "- Perturbations Generate & Filter (Character, Word Level)\n",
    "- Sentence Embedding\n",
    "\n",
    "**3. Subspace Generation**\n",
    "- Hyper-Rectangle, Subspace Generation (Semantic & Geometric)\n",
    "\n",
    "**4. Classifier Training**\n",
    "- ?? PGD\n",
    "- Classifier NN Models Training\n",
    "\n",
    "**5. Verification**\n",
    "- ??\n",
    "\n",
    "**6. Classifier Evaluation (Performance, Robustness ....)**\n",
    "- Models Evaluation & Comparison (Clean, Adversarial Attack)\n",
    "- ?? Performance\n",
    "- ?? Robustness\n",
    "- ?? Verifiability\n",
    "- ?? Generalisability\n",
    "- ?? Falsifiability\n",
    "\n",
    "### Note: ANTONIO\n",
    "\n",
    "- ANTONIO Parametric NLP Verification Pipeline https://github.com/ANTONIONLP/ANTONIO/tree/main\n",
    "- Casadio, M. et al. (2025) ‘NLP verification: towards a general methodology for certifying robustness’, European Journal of Applied Mathematics. 2025/04/02 edn, pp. 1–58. Available at: https://doi.org/10.1017/S0956792525000099.\n",
    "\n",
    "\n",
    "<img src=\"ANTONIO_Pipeline.png\" alt=\"ANTONIO Pipeline\" style=\"width:900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539e770",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tensorflow, Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Installs\n",
    "#\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print()\n",
    "\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Unknown')\n",
    "print(f\"Conda Environment: {conda_env}\")\n",
    "print()\n",
    "\n",
    "!pip list | grep -E \"(pandas|numpy|matplotlib|scikit-learn|tensorflow|tensorflow-metal|keras-tuner|sentence-transformers)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92929403",
   "metadata": {},
   "source": [
    "***\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271409c4",
   "metadata": {},
   "source": [
    "## Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Run Setup\n",
    "#\n",
    "\n",
    "def run_setup(local_project_folder, run_name=''):\n",
    "\n",
    "    # Folder paths\n",
    "    data_folder = local_project_folder.joinpath('data')\n",
    "    if not data_folder.exists():\n",
    "        raise FileNotFoundError(f'{data_folder} does not exist')\n",
    "    results_folder = local_project_folder.joinpath('run_results')\n",
    "    if not results_folder.exists():\n",
    "        raise FileNotFoundError(f'{results_folder} does not exist')\n",
    "\n",
    "    # Run Results\n",
    "    run_name = f'{run_name.strip()}'\n",
    "    run_name = f'Run_{run_name}' if (run_name) else f'Run_{datetime.now().strftime(\"%Y%m%d\")}'\n",
    "    run_results_folder = results_folder.joinpath(f'{run_name}')\n",
    "    run_results_folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    return data_folder, run_results_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511396a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Source Datsets, Combine and Create df\n",
    "#\n",
    "\n",
    "def get_query_data(data_folder, run_results_folder):\n",
    "    \"\"\"\n",
    "    Combines two datsets & saves it\n",
    "    - medicheck-expert.csv using expert classifications 0: as 'Non-Medical' (1) and 1,2,3 as 'Medical' (0)\n",
    "    - medicheck-neg.csv as 'Non-Medical'\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        questions.df: df with sentenaces and classification label\n",
    "    \"\"\"\n",
    "\n",
    "    # Load two datasets & combine\n",
    "    # NB query_label_expert = 999 denotes no expert labeling\n",
    "    expert_df = pd.read_csv(data_folder.joinpath('medicheck-expert.csv'))\n",
    "    neg_df = pd.read_csv(data_folder.joinpath('medicheck-neg.csv'), header=None, names = ['query', 'query-label-expert'], escapechar='\\\\')\n",
    "    neg_df['query-label-expert'] = 999\n",
    "\n",
    "    # Combine the two datasets\n",
    "    expert_df = expert_df[['query', 'query-label-expert']]\n",
    "    combined_df = pd.concat([expert_df, neg_df], ignore_index=True)\n",
    "\n",
    "    # Classify the querys into types\n",
    "    # 'Non-Medical' (1) and 'Medical' (0)\n",
    "    combined_df['query-is-non-medical'] = combined_df['query-label-expert'].isin([0,999])\n",
    "\n",
    "    # Add a unique ref-id for later back-tracking, add data source\n",
    "    combined_df.insert(0, 'ref-id', [str(uuid.uuid4())[:8] for _ in range(len(combined_df))])\n",
    "    combined_df.insert(1, 'source', 'clean')\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103904e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a query df\n",
    "#\n",
    "\n",
    "def inspect_query_df(query_df):\n",
    "    print(f\"Full Dataset Shape: {query_df.shape}\")\n",
    "    display(query_df.head(10))\n",
    "\n",
    "    # Plot Categories\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "\n",
    "    query_df['query-label-expert'].value_counts().sort_index().plot(kind='bar', ax=axs[0])\n",
    "    axs[0].set_title('query-label-expert')\n",
    "    axs[0].set_xlabel('0=Non-medical, 1=Non-serious, 2=Serious, 3=Critical, 999=Non-Expert')\n",
    "    axs[0].set_ylabel('Count')\n",
    "    axs[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    query_df['query-is-non-medical'].value_counts().plot(kind='bar', ax=axs[1], color=['red', 'green'])\n",
    "    axs[1].set_title('query-is-non-medical')\n",
    "    axs[1].set_xlabel('True: 0,99, False: 1,2,3')\n",
    "    axs[1].set_ylabel('Count')\n",
    "    axs[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10075e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence embedding function\n",
    "#\n",
    "\n",
    "def create_embeddings(sentences):\n",
    "    # TODO: Understand why vector allignment is needed and how it is performed, see Antonio\n",
    "    # See https://www.kaggle.com/discussions/general/566301\n",
    "\n",
    "    # SBERT with all-MiniLM-L6-V2, a small and fast model, 385 dimensions\n",
    "    # ANTONIO: encoding_models = {'all-MiniLM-L6-v2': 'sbert22M'}\n",
    "    sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = sbert_model.encode(sentences, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    # Allign vectors to the axis and rotate\n",
    "    u, s, vh = np.linalg.svd(a=embeddings)\n",
    "    align_mat = np.linalg.solve(a=vh, b=np.eye(len(embeddings[0])))\n",
    "    embeddings = np.matmul(embeddings, align_mat)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training etc Data\n",
    "#\n",
    "\n",
    "def prep_data_splits(queries_df):\n",
    "    \"\"\"\n",
    "    Prepare X,y with Train, Val, Test\n",
    "    !! Assumes no K-Fold validation, and need to split before PCA\n",
    "\n",
    "    Args:\n",
    "        Queries_df, sentences and embeddings: Pandas df\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train etc: np ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    # Select features, X and target y\n",
    "    X = np.vstack(queries_df['query-embedding'].values)\n",
    "    y = queries_df['query-is-non-medical'].to_numpy().astype(np.float32)  # Convert bool to float32\n",
    "\n",
    "    # Ref for later mapping back to original query test, ie before the splits\n",
    "    ref_ids = queries_df['ref-id'].values\n",
    "\n",
    "    # Train, validate, test split 60:20:20\n",
    "    # NB: Classes are possibly imbalanced so use stratification\n",
    "    X_train, X_temp, y_train, y_temp, ids_train, ids_temp = train_test_split(X, y, ref_ids, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test, ids_val, ids_test = train_test_split(X_temp, y_temp, ids_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    print(f\"Full queries df shape: {queries_df.shape}\")\n",
    "    print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "    print(f\"Ref ids: {ref_ids.shape}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on embeddings\n",
    "#\n",
    "\n",
    "def pca_create(X_train, X_val, X_test, components=30):\n",
    "\n",
    "    # TODO: components=30 reduces to 41% explained variance ... but better performing model\n",
    "\n",
    "    # Repeatable Pipeline, with scaling\n",
    "    pipe = Pipeline([\n",
    "        (\"center\", StandardScaler(with_mean=True, with_std=False)),\n",
    "        # (\"pca\", PCA(n_components=0.75, svd_solver=\"full\", random_state=42)),\n",
    "        (\"pca\", PCA(n_components=components, svd_solver=\"full\", random_state=42)), \n",
    "    ])\n",
    "\n",
    "    # Fit to X_train and transform train, val, test\n",
    "    train_PCA_pipe = pipe.fit(X_train)\n",
    "    X_train_pca = train_PCA_pipe.transform(X_train)\n",
    "    X_val_pca = train_PCA_pipe.transform(X_val)\n",
    "    X_test_pca = train_PCA_pipe.transform(X_test)\n",
    "    # X_train_pca = pipe.fit_transform(X_train)\n",
    "    # X_val_pca = pipe.transform(X_val)\n",
    "    # X_test_pca = pipe.transform(X_test)\n",
    "\n",
    "    print('Dimensions after PCA. Train, val, test')\n",
    "    print(\"Original\", X_train.shape[1], X_val.shape[1], X_test.shape[1])\n",
    "    print(\"Reduced\", X_train_pca.shape[1], X_val_pca.shape[1], X_test_pca.shape[1])\n",
    "    print(f'Explained variance total: {pipe.named_steps[\"pca\"].explained_variance_ratio_.sum():.4f}')\n",
    "\n",
    "    cum_var = np.cumsum(pipe.named_steps[\"pca\"].explained_variance_ratio_)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(cum_var)+1), cum_var, marker='o')\n",
    "    plt.axhline(0.90, color='red', linestyle='--', linewidth=1)  # 90% threshold\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.title('Cumulative explained variance by PCA components')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Save pipeline for later use with the NN model\n",
    "    # import joblib\n",
    "    # joblib.dump(pipe, \"pca_embeddings_pipeline.joblib\")\n",
    "    # Reload with NN model\n",
    "    # pipe = joblib.load(\"pca_embeddings_pipeline.joblib\")\n",
    "    # X_new_pca = pipe.transform(X_new)\n",
    "    \n",
    "    return train_PCA_pipe, X_train_pca, X_val_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tf Datasets\n",
    "#\n",
    "\n",
    "def prep_tf_datasets(X_train, X_val, y_train, y_val, batch_size):\n",
    "\n",
    "    # Dimensions\n",
    "    embedding_dimensions = X_train.shape[1]\n",
    "\n",
    "    # Dreate tf datasets\n",
    "    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, y_train.astype(np.float32)))\n",
    "    val_dataset_tf = tf.data.Dataset.from_tensor_slices((X_val, y_val.astype(np.float32)))\n",
    "    # test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test_pca, y_test.astype(np.float32)))\n",
    "\n",
    "    # Shuffle, batch etc to improve gradient descent, efficiency ...\n",
    "    buffer_size = max(1024, len(X_train) + len(X_val))\n",
    "\n",
    "    train_dataset_tf = train_dataset_tf.shuffle(buffer_size=buffer_size)\n",
    "    train_dataset_tf = train_dataset_tf.batch(batch_size)\n",
    "    val_dataset_tf = val_dataset_tf.batch(batch_size)\n",
    "    val_dataset_tf = val_dataset_tf.prefetch(1)\n",
    "\n",
    "    print(train_dataset_tf)\n",
    "    print(val_dataset_tf)\n",
    "\n",
    "    return train_dataset_tf, val_dataset_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tf Datasets\n",
    "#\n",
    "\n",
    "def prep_tf_datasets(X_train, X_val, y_train, y_val, batch_size):\n",
    "\n",
    "    # Dreate tf datasets\n",
    "    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, y_train.astype(np.float32)))\n",
    "    val_dataset_tf = tf.data.Dataset.from_tensor_slices((X_val, y_val.astype(np.float32)))\n",
    "    # test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test_pca, y_test.astype(np.float32)))\n",
    "\n",
    "    # Shuffle, batch etc to improve gradient descent, efficiency ...\n",
    "    buffer_size = max(1024, len(X_train) + len(X_val))\n",
    "    # batch_size = 64\n",
    "\n",
    "    train_dataset_tf = train_dataset_tf.shuffle(buffer_size=buffer_size)\n",
    "    train_dataset_tf = train_dataset_tf.batch(batch_size)\n",
    "    val_dataset_tf = val_dataset_tf.batch(batch_size)\n",
    "    val_dataset_tf = val_dataset_tf.prefetch(1)\n",
    "\n",
    "    print('tf Datasets')\n",
    "    print(train_dataset_tf)\n",
    "    print(val_dataset_tf)\n",
    "\n",
    "    return train_dataset_tf, val_dataset_tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a2103",
   "metadata": {},
   "source": [
    "## Perturbation, Adversarial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random hyperrectangles based on embeddings\n",
    "#\n",
    "\n",
    "def generate_hyperrectangles(hyper_type, embedding_data):\n",
    "\n",
    "    if hyper_type == 'eps_cube':\n",
    "        raise Exception(f'{hyper_type} not implemented')\n",
    "        # TODO: Implement eps_cube\n",
    "    \n",
    "    elif hyper_type == 'cos_sim':\n",
    "        raise Exception(f'{hyper_type} not implemented')\n",
    "        # TODO: Implement cosine similarity\n",
    "\n",
    "    elif hyper_type == 'min_max':\n",
    "        num_rectangles = len(embedding_data) \n",
    "\n",
    "        # Min and max bounds for each dimension\n",
    "        min_bounds = np.min(embedding_data, axis=0)\n",
    "        max_bounds = np.max(embedding_data, axis=0)\n",
    "        dimensions_num = len(min_bounds)\n",
    "\n",
    "        # Empty Bounds\n",
    "        lower_bounds = np.empty((num_rectangles, dimensions_num))\n",
    "        upper_bounds = np.empty((num_rectangles, dimensions_num))\n",
    "        \n",
    "        # Create hyperrectangles, with random bounds\n",
    "        hyperrectangles = np.empty((num_rectangles, dimensions_num, 2))\n",
    "        for rect_idx in range(num_rectangles):\n",
    "            lower = min_bounds + np.random.random(dimensions_num) * (max_bounds - min_bounds) * 0.5\n",
    "            upper = lower + np.random.random(dimensions_num) * (max_bounds - lower) * 0.9\n",
    "\n",
    "            for dim_idx in range(dimensions_num):\n",
    "                hyperrectangles[rect_idx, dim_idx] = [lower[dim_idx], upper[dim_idx]]\n",
    "\n",
    "            lower_bounds[rect_idx] = lower\n",
    "            upper_bounds[rect_idx] = upper  \n",
    "\n",
    "    else:\n",
    "        raise Exception(f'{hyper_type} not valid')\n",
    "    \n",
    "    print(f'Hyperrectangles Creation - {hyper_type} shape: {hyperrectangles.shape}')\n",
    "\n",
    "    return hyperrectangles, (lower_bounds, upper_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8384c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PGD Attack Generation - Using Hyperrectangles\n",
    "#\n",
    "\n",
    "# TODO: Review approach below, is it sufficient?\n",
    "\n",
    "def pgd_attack_batch(model, X_batch, y_batch, pgd_steps, eps, gamma):\n",
    "    \"\"\"\n",
    "    Performs PGD attack on a batch of inputs.\n",
    "\n",
    "    Args:\n",
    "        model: The model to attack.\n",
    "        x_batch: Batch of input data.\n",
    "        y_batch: Batch of true labels.\n",
    "        pgd_steps: Number of PGD steps.\n",
    "        eps: Tuple containing (lower bounds, upper bounds) for each dimension from hyperrectangles.\n",
    "        gamma: Step size for each PGD iteration.\n",
    "\n",
    "    Returns:\n",
    "        perturbed_x: Adversarial examples generated from the input batch.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Make a copy of the input to avoid modifying the original data\n",
    "    X_adv = tf.identity(X_batch)\n",
    "    \n",
    "    # Iterate through the PGD steps\n",
    "    for _ in range(pgd_steps):\n",
    "        # Forward pass - calc loss and retain in tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(X_adv)              # Records all operations, watches for gradient calc\n",
    "            predictions = model(X_adv, training=False)\n",
    "            loss = loss_fn(y_batch, predictions)\n",
    "            # loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, predictions)\n",
    "        \n",
    "        # Compute gradient of the loss w.r.t the input\n",
    "        gradients = tape.gradient(loss, X_adv)\n",
    "        \n",
    "        # Apply perturbation based on sign of the gradient\n",
    "        # Perform gradient ascent step in the direction that maximizes the loss\n",
    "        # TODO: Better understand how this maximises the loss?\n",
    "        signed_grad = tf.sign(gradients)\n",
    "        X_adv = X_adv + gamma * signed_grad\n",
    "\n",
    "        # Project back within the epsilon bounds,\n",
    "        # - the hyperrectangle with upper and lower bounds for each dimension\n",
    "        # - keep to within 0 and 1\n",
    "        X_adv = tf.clip_by_value(X_adv, eps[0], eps[1])\n",
    "        # X_adv = tf.clip_by_value(X_adv, 0.0, 1.0) \n",
    "        # TODO: Is this appropriate for embedding vectors that may be outside these bounds??\n",
    "\n",
    "    return X_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PGD dataset for use in model training\n",
    "#\n",
    "\n",
    "# TODO: This is as per Marco's NLP code but NOT ANTONIO\n",
    "\n",
    "def generate_PGD_dataset(X, y, model, pgd_steps, gamma_multiplier, batch_size, hyper_type):\n",
    "\n",
    "    # pgd_dataset = []\n",
    "    pgd_tensors = []\n",
    "    pgd_labels = []\n",
    "\n",
    "    # # Extract only the positive X, y, ie non-medical\n",
    "    # mask_test_pos = (y == 1)\n",
    "    # X = X[mask_test_pos]\n",
    "    # y = y[mask_test_pos]\n",
    "\n",
    "    # Generate hyperrectangles and combine into a tf dataset\n",
    "    # n * dimensions with upper/lower bounds\n",
    "    hyperrectangles, hyper_bounds = generate_hyperrectangles(hyper_type, X)       \n",
    "    hyperrectangles_labels = y\n",
    "    hyperrectangles = tf.convert_to_tensor(hyperrectangles, dtype=tf.float32)\n",
    "    # hyperrectangles_labels = tf.convert_to_tensor(hyperrectangles_labels, dtype=tf.int64)\n",
    "\n",
    "    for i, hyperrectangle in enumerate(hyperrectangles):\n",
    "        eps = tf.transpose(hyperrectangle)   # For each dimension, the hyperrectangle lower and upper bounds\n",
    "        gamma = tf.expand_dims((eps[1] - eps[0]) / (pgd_steps * gamma_multiplier), axis=0)  # Step-size within the bounds\n",
    "\n",
    "        # Generate random point(s) in hyperrectangles\n",
    "        # pgd_point = tf.random.uniform(shape=[1, eps.shape[-1]], minval=eps[0], maxval=eps[1])\n",
    "        tf.random.set_seed(42)\n",
    "        pgd_point = tf.random.uniform(shape=(1, eps.shape[-1]), minval=eps[0], maxval=eps[1], dtype=tf.float32)\n",
    "        pgd_label = tf.expand_dims(hyperrectangles_labels[i], axis=0)\n",
    "\n",
    "        # Generate the PGD attacks\n",
    "        pgd_point_adv = pgd_attack_batch(model, pgd_point, pgd_label, pgd_steps, eps, gamma)\n",
    "        pgd_tensors.append(pgd_point_adv)    \n",
    "        pgd_labels.append(pgd_label)\n",
    "        # if len(pgd_dataset) > 0:\n",
    "        #     pgd_dataset = tf.concat([pgd_dataset, pgd_point], axis=0)\n",
    "        #     pgd_labels = tf.concat([pgd_labels, pgd_label], axis=0)\n",
    "        # else:\n",
    "        #     pgd_dataset = pgd_point\n",
    "        #     pgd_labels = pgd_label\n",
    "\n",
    "    pgd_dataset = tf.concat(pgd_tensors, axis=0)  \n",
    "    pgd_labels = tf.concat(pgd_labels, axis=0) \n",
    "\n",
    "    # Convert into tf datasets, shuffle and batch them\n",
    "    pgd_dataset_tf = tf.data.Dataset.from_tensor_slices((pgd_dataset, pgd_labels))\n",
    "    pgd_dataset_tf = pgd_dataset_tf.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    # Also return np array X,y\n",
    "    arrs = list(pgd_dataset_tf.as_numpy_iterator())   # list of (X_batch, y_batch)\n",
    "    X = np.concatenate([b[0] for b in arrs], axis=0)\n",
    "    y = np.concatenate([b[1] for b in arrs], axis=0)\n",
    "\n",
    "    return pgd_dataset_tf, X, y, hyper_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45b1c1",
   "metadata": {},
   "source": [
    "## xx Sentence Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435be02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Perturbations Using the 'Clean' Sentences\n",
    "#\n",
    "\n",
    "def create_perturbations(clean_df):\n",
    "\n",
    "    # Generate Perturbations\n",
    "    # Character, word, sentence level\n",
    "\n",
    "    # Generate similar sentenaces using an LLM\n",
    "    # TODO: Vicuna is very large and slow so using a small OpenAI model for testing\n",
    "    for idx, sentence in clean_df.iterrows():\n",
    "        if idx >= 1:\n",
    "            break\n",
    "        sentence_dict = dict(zip(clean_df.columns, sentence))\n",
    "        print(sentence_dict)\n",
    "\n",
    "        # print(sentence['ref-id'], sentence['query'])\n",
    "        new_sentences = gen_sentences(sentence_dict['query'])\n",
    "        for new_sentence in new_sentences:\n",
    "            print(new_sentence)\n",
    "    \n",
    "\n",
    "\n",
    "    # Filter Pertubations by Validity\n",
    "    # Semantic and grammaticaly similar\n",
    "    # Medical/non-medical label not invalid/changed\n",
    "    # TODO: use cosine similarity\n",
    "    # TODO: use ROUGE-N\n",
    "\n",
    "    \n",
    "    # TODO: Complete creation\n",
    "    print('Placeholder - Perturbations')\n",
    "    perturbations_df = clean_df.copy()\n",
    "\n",
    "    return perturbations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Call to Get Sentences\n",
    "#\n",
    "\n",
    "import openai\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "default_model = \"gpt-4o-mini\"\n",
    "default_temperature = 0.8\n",
    "\n",
    "import re\n",
    "\n",
    "def gen_sentences(clean_sentence):\n",
    "\n",
    "    sentence_count = 5\n",
    "    open_prompt = f\"Rephrase this sentence {sentence_count} times. Return each on a new line: {clean_sentence}\"\n",
    "    responses = openai.chat.completions.create(\n",
    "        model=default_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": open_prompt},\n",
    "        ],\n",
    "        temperature=default_temperature,\n",
    "    )\n",
    "    raw_output = responses.choices[0].message.content\n",
    "    sentences = [s.strip() for s in raw_output.split('\\n') if s.strip()]\n",
    "    cleaned_sentences = [re.sub(r'^\\d+[.\\s]*', '', s) for s in sentences]\n",
    "\n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61492",
   "metadata": {},
   "source": [
    "## NN Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple Base Model Build\n",
    "# # \n",
    "\n",
    "# def build_model(input_size, model_name):\n",
    "\n",
    "#     initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "#     model = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Input(shape=(input_size)),\n",
    "#             tf.keras.layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "#             tf.keras.layers.Dense(2, activation='linear', kernel_initializer=initializer, name='output')\n",
    "#             # tf.keras.layers.Dense(2, activation='softmax', kernel_initializer=initializer, name='output')\n",
    "#         ], name=model_name)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# Simple Base Model Build & Compile\n",
    "# \n",
    "\n",
    "def build_compile_model(input_size, model_name):\n",
    "\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(input_size)),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(2, activation='linear', kernel_initializer=initializer, name='output')\n",
    "            # tf.keras.layers.Dense(2, activation='softmax', kernel_initializer=initializer, name='output')\n",
    "        ], name=model_name)\n",
    "    \n",
    "    # Set paramaters and compile\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    eval_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=[eval_metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Model Builder - hyperparameter tuning with Keras Tuner\n",
    "#\n",
    "\n",
    "def model_builder(hp, input_size, model_name):\n",
    "\n",
    "    model = tf.keras.Sequential(name=model_name)\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.Input(shape=(input_size,)))\n",
    "\n",
    "    # Hidden layers - number of layers and neurons etc per layer is tuneable\n",
    "    num_layers = hp.Int('num_layers', min_value=0, max_value=3)\n",
    "    for i in range(num_layers):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units = hp.Int(f'units_{i}', min_value=8, max_value=128, step=8),\n",
    "            activation = hp.Choice(f'activation_{i}', ['relu', 'tanh', 'elu']),\n",
    "            kernel_initializer=initializer))\n",
    "    \n",
    "    # Add regularisaration to last layer\n",
    "    if hp.Boolean('dropout'):\n",
    "        model.add(tf.keras.layers.Dropout(\n",
    "            rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1),\n",
    "            seed=42))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(2, \n",
    "        activation='linear', \n",
    "        kernel_initializer=initializer,\n",
    "        name='output'))\n",
    "\n",
    "    # Optimiser and learning rate\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])    # default:0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss function and metrics\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    # metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    # metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "\n",
    "    # Build the model\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Inspect hyperparameters after tuning\n",
    "#\n",
    "\n",
    "def inspect_hypperparams(hpys):\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    print(f\"Number of layers: {hpys.get('num_layers')}\")\n",
    "    print(f\"Learning rate: {hpys.get('learning_rate')}\")\n",
    "    print(f\"Dropout: {hpys.get('dropout')}\")\n",
    "    if hpys.get('dropout'):\n",
    "        print(f\"Dropout rate: {hpys.get('dropout_rate')}\")\n",
    "\n",
    "    # Print units and activation for each layer\n",
    "    num_layers = hpys.get('num_layers')\n",
    "    for i in range(num_layers):\n",
    "        print(f\"Layer {i}: units={hpys.get(f'units_{i}')}, activation={hpys.get(f'activation_{i}')}\")\n",
    "\n",
    "\n",
    "# Run the hyperparameterstuning\n",
    "#\n",
    "\n",
    "def get_tuned_model(input_dimensions, epochs, model_name, results_folder, train_dataset_tf, val_dataset_tf):\n",
    "\n",
    "    # TODO: Change objective to use MCC or Precision to better catch False Positives etc?\n",
    "\n",
    "    # Hyperparameter search\n",
    "    # tuner = kt.Hyperband(model_builder,\n",
    "    tuner = kt.Hyperband(lambda hp: model_builder(hp, input_size=input_dimensions, model_name=model_name),\n",
    "                        objective='val_sparse_categorical_accuracy',\n",
    "                        max_epochs=10,\n",
    "                        factor=2,\n",
    "                        directory=results_folder,\n",
    "                        project_name='kt_hyper_tuning')\n",
    "    tuner.search(train_dataset_tf, epochs=epochs, validation_data=val_dataset_tf)\n",
    "\n",
    "    # Best Hyperparameters inspect\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    inspect_hypperparams(best_hps)\n",
    "\n",
    "    # Fully train using the best hyperparameters\n",
    "    tuned_model = model_builder(best_hps, input_size=input_dimensions, model_name=model_name)\n",
    "\n",
    "    return tuned_model\n",
    "\n",
    "\n",
    "# Create a deep copy of a Keras model\n",
    "#\n",
    "\n",
    "import tempfile, shutil\n",
    "from typing import cast\n",
    "\n",
    "def model_copy(model):\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    file_path = Path(tempfile.mkdtemp()).joinpath('temp.keras')\n",
    "    model.save(file_path)\n",
    "    model_copy = keras.models.load_model(file_path)\n",
    "\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "    return cast(tf.keras.Model, model_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model - Custom Loop\n",
    "# For 'clean' data training and also adversarial training\n",
    "#\n",
    "\n",
    "def custom_model_train(model, epochs, train_dataset, val_dataset,\n",
    "                       alpha, \n",
    "                       beta=0.0, pgd_steps=5, gamma_multiplier=1000, batch_size=8, hyper_type = 'min_max'):\n",
    "\n",
    "    # TODO: NB using same loss function and metrics for clean and PGD training\n",
    "    # TODO: Clarify use of training and validation data ...\n",
    "\n",
    "    # Model parameters\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    loss_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_metric_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_metric_pgd = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    eval_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    eval_metric_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    eval_metric_pgd = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Train for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Epoch {epoch + 1} of {epochs}. \")\n",
    "        eval_metric.reset_states()\n",
    "        eval_metric_val.reset_states()\n",
    "        eval_metric_pgd.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        loss_metric_val.reset_states()\n",
    "        loss_metric_pgd.reset_states()\n",
    "\n",
    "        # For each batch in the datset\n",
    "        for step, (X_batch, y_batch) in enumerate(train_dataset):\n",
    "            # 1. Clean X training\n",
    "            #\n",
    "\n",
    "            # Forward pass - calc loss and retain in tape\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(X_batch, training=True)\n",
    "                loss = loss_fn(y_batch, logits) * alpha\n",
    "            \n",
    "            # Backpropagate - gradient descent and optimised model weights\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # Update metrics\n",
    "            loss_metric.update_state(y_batch, logits)\n",
    "            eval_metric.update_state(y_batch, logits)\n",
    "\n",
    "            # 2. PGD Attack Training\n",
    "            #\n",
    "\n",
    "            # if beta > 0 and gen_PGD_dataset != None:\n",
    "            if beta > 0:\n",
    "                pgd_dataset, _, _,_ = generate_PGD_dataset(X_batch, y_batch, model, pgd_steps, gamma_multiplier, batch_size, hyper_type)\n",
    "\n",
    "                for step, (X_batch, y_batch) in enumerate(pgd_dataset):\n",
    "                    # print(f'PGD step: {step}')\n",
    "                    # Forward pass - calc loss and retain in tape\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        pgd_logits = model(X_batch, training=True)\n",
    "                        pgd_loss = loss_fn(y_batch, pgd_logits) * beta\n",
    "\n",
    "                    # Backpropagate - gradient descent and optimised model weights\n",
    "                    pgd_gradients = tape.gradient(pgd_loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(pgd_gradients, model.trainable_variables))\n",
    "\n",
    "                    # Update metrics\n",
    "                    loss_metric_pgd.update_state(y_batch, pgd_logits)\n",
    "                    eval_metric_pgd.update_state(y_batch, pgd_logits)\n",
    "\n",
    "        # End of epoch\n",
    "        # Validation results calc\n",
    "        for X_val_batch, y_val_batch in val_dataset:\n",
    "            val_logits = model(X_val_batch, training=False)\n",
    "            loss_metric_val.update_state(y_val_batch, val_logits)\n",
    "            eval_metric_val.update_state(y_val_batch, val_logits)\n",
    "\n",
    "        loss_metric_value = loss_metric.result().numpy()\n",
    "        loss_metric_val_value = loss_metric_val.result().numpy()\n",
    "        loss_metric_pgd_value = loss_metric_pgd.result().numpy() \n",
    "\n",
    "        metric_name = eval_metric.name\n",
    "        metric_value = eval_metric.result().numpy()\n",
    "        metric_val_value = eval_metric_val.result().numpy()\n",
    "        metric_pgd_value = eval_metric_pgd.result().numpy()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # print(f'Training: Loss: {loss_metric_value:4f} Metric: {metric_name} - Train: {metric_value:.4f} & Val: {val_metric_value:.4f} Time: {elapsed_time:.2f}s')\n",
    "        print(f'Training Results. Eval metric: {metric_name} Time: {elapsed_time:.2f}s')\n",
    "        print(f'Training Clean: Loss: {loss_metric_value:4f} Eval Train {metric_value:.4f}')\n",
    "        print(f'Training PGD: Loss: {loss_metric_pgd_value:4f} Eval Train {metric_pgd_value:.4f}')\n",
    "        print(f'Validation: Loss: {loss_metric_val_value:4f} Eval Train {metric_val_value:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Details\n",
    "#\n",
    "\n",
    "def inspect_model(model):\n",
    "\n",
    "    print(f'\\n{model.summary()}')    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"\\nLayer {i}: {layer.name} ({type(layer).__name__})\")\n",
    "        print(f\"  - Input shape: {layer.input_shape}\")\n",
    "        print(f\"  - Output shape: {layer.output_shape}\")\n",
    "        \n",
    "        # Show activation function for layers that have it\n",
    "        if hasattr(layer, 'activation'):\n",
    "            print(f\"  - Activation: {layer.activation.__name__}\")\n",
    "            \n",
    "        # Show units for Dense layers\n",
    "        if hasattr(layer, 'units'):\n",
    "            print(f\"  - Units: {layer.units}\")\n",
    "            \n",
    "        # Show other layer-specific attributes\n",
    "        if hasattr(layer, 'kernel_initializer') and layer.kernel_initializer is not None:\n",
    "            print(f\"  - Kernel initializer: {layer.kernel_initializer.__class__.__name__}\")\n",
    "            \n",
    "        # Show parameter count for this layer\n",
    "        if hasattr(layer, 'count_params'):\n",
    "            print(f\"  - Parameters: {layer.count_params():,}\")\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "#\n",
    "\n",
    "def evaluate_model(model, X, y, run_name='base', data_source='clean', verbose=False):\n",
    "\n",
    "    # Model predictions\n",
    "    start_time = time.perf_counter()\n",
    "    y_pred_probs = model.predict(X)\n",
    "    # y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1) \n",
    "    duration = time.perf_counter() - start_time\n",
    "\n",
    "    # Calculate Metrics\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = metrics.accuracy_score(y_true=y, y_pred=y_pred)\n",
    "    recall = metrics.recall_score(y_true=y, y_pred=y_pred, pos_label=1)\n",
    "    precision = metrics.precision_score(y_true=y, y_pred=y_pred, pos_label=1)\n",
    "    f1_score = metrics.f1_score(y_true=y, y_pred=y_pred, pos_label=1)\n",
    "    specificity = tn / (tn + fp)\n",
    "    mcc = matthews_corrcoef(y, y_pred)\n",
    "\n",
    "    # Print Metrics\n",
    "    if verbose:\n",
    "        print('\\n------------------------------------------------')\n",
    "        print(f'Evaluation Results for: {run_name}, {model.name} on {data_source} data')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'Recall (Sensitivity, TP Rate): {recall:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Specificity (1 - Fall-Out): {specificity:.4f}')\n",
    "        print(f'F1 Score {f1_score:.4f}')\n",
    "        print(f'MCC: {mcc:.4f}')\n",
    "        print('-----------')\n",
    "        print(f'Fall Out (FPR): {fp / (fp + tn):.4f}')\n",
    "        print(f'Hamming Loss {metrics.hamming_loss(y_true=y, y_pred=y_pred):.4f}')\n",
    "        # roc_auc_score = metrics.roc_auc_score(y_true=y_test, y_score=y_pred_probs.flatten())\n",
    "        roc_auc_score = metrics.roc_auc_score(y_true=y, y_score=y_pred_probs[:, 1])\n",
    "        print(f'ROC-AUC Score {roc_auc_score:.4f}')\n",
    "        gini_score = 2 * roc_auc_score - 1\n",
    "        print(f'Gini Index: {gini_score:.4f}')\n",
    "        print('---------------------------------------------')\n",
    "\n",
    "    # Plot Confusion Matrix & ROC Curve\n",
    "    if verbose:\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "        fig.suptitle(f'Model Prediction Evaluation', fontsize=20)\n",
    "\n",
    "        axes[0].set_title('Confusion Matrix')    \n",
    "        # class_labels = ['Unsafe', 'Safe']\n",
    "        # class_labels = ['Non-Medical', 'Medical']\n",
    "        class_labels = ['Medical', 'Non-Medical']\n",
    "        ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels).plot(ax=axes[0])\n",
    "        axes[1].set_title('ROC Curve')\n",
    "        # roc_display = RocCurveDisplay.from_estimator(model, X_test, y_test, ax=axes[1], pos_label=1)\n",
    "        # RocCurveDisplay.from_predictions(y_test, y_pred_probs.flatten(), ax=axes[1], pos_label=1)\n",
    "        RocCurveDisplay.from_predictions(y, y_pred_probs[:, 1], ax=axes[1], pos_label=1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.style.use('ggplot')\n",
    "\n",
    "    # Return Results\n",
    "    results = {'run_name': run_name,\n",
    "               'model_name': model.name,\n",
    "               'data_source': data_source,\n",
    "               'prediction_time': duration,\n",
    "               'accuracy': accuracy,\n",
    "               'recall': recall,\n",
    "               'precision': precision,\n",
    "               'specificity': specificity,\n",
    "               'f1_score': f1_score,\n",
    "               'mcc': mcc\n",
    "               }\n",
    "    \n",
    "    return results\n",
    "\n",
    "#   Evaluate Model Robustness, Effectiveness of an Adversarial Attack\n",
    "#\n",
    "\n",
    "def evaluate_model_attack(model, X_clean, y_clean, X_adv, y_adv, \n",
    "                          bounds,\n",
    "                          verbose=False):\n",
    "        \"\"\"\n",
    "        Analyze the effectiveness of an adversarial attack\n",
    "        \"\"\"\n",
    "\n",
    "        # Get predictions for clean, adversarial\n",
    "        y_pred_probs_clean = model.predict(X_clean)\n",
    "        y_pred_clean = np.argmax(y_pred_probs_clean, axis=1)\n",
    "        y_pred_probs_adv = model.predict(X_adv)\n",
    "        y_pred_adv = np.argmax(y_pred_probs_adv, axis=1)\n",
    "\n",
    "        # Models Base Metrics\n",
    "        accuracy_clean = metrics.accuracy_score(y_true=y_clean, y_pred=y_pred_clean)\n",
    "        cm_clean = confusion_matrix(y_clean, y_pred_clean)\n",
    "        tn_clean, fp_clean, fn_clean, tp_clean = cm_clean.ravel()\n",
    "        total_clean = tn_clean + fp_clean + fn_clean + tp_clean\n",
    "        specificity_clean = tn_clean / (tn_clean + fp_clean)\n",
    "\n",
    "        accuracy_adv = metrics.accuracy_score(y_true=y_adv, y_pred=y_pred_adv)\n",
    "        cm_adv = confusion_matrix(y_adv, y_pred_adv)\n",
    "        tn_adv, fp_adv, fn_adv, tp_adv = cm_adv.ravel()\n",
    "        total_adv = tn_adv + fp_adv + fn_adv + tp_adv\n",
    "        specificity_adv = tn_adv / (tn_adv + fp_adv)\n",
    "\n",
    "        # Model Robustness\n",
    "        # Attack Success Rate (ASR) - The percentage of adversarial examples that successfully change the model's prediction\n",
    "        attack_success_rate = np.mean(y_pred_clean != y_pred_adv)\n",
    "        robustness_gap_acc = accuracy_clean - accuracy_adv\n",
    "        robustness_gap_sp = specificity_clean - specificity_adv\n",
    "\n",
    "        # Perturbation statistics\n",
    "        perturbation = X_adv - X_clean\n",
    "        lower_bounds, upper_bounds = bounds\n",
    "        perturbation_ranges = upper_bounds - lower_bounds\n",
    "        # bound_utilization = np.abs(perturbation) / perturbation_ranges\n",
    "        bound_utilization = np.abs(perturbation) / (perturbation_ranges + 1e-12)  # avoid div-zero\n",
    "        avg_bound_utilization = np.mean(bound_utilization)\n",
    "\n",
    "        linf_norm = np.mean(np.max(np.abs(perturbation), axis=1))\n",
    "        l2_norm = np.mean(np.linalg.norm(perturbation, axis=1))\n",
    "        l0_norm = np.sum(np.abs(perturbation) > 1e-8, axis=1)\n",
    "        l0_mean = np.mean(l0_norm)\n",
    "\n",
    "        # Per-dimension analysis\n",
    "        dim_perturbation_stats = {\n",
    "            'mean_abs_perturbation': np.mean(np.abs(perturbation), axis=0),\n",
    "            'max_abs_perturbation': np.max(np.abs(perturbation), axis=0),\n",
    "            'perturbation_std': np.std(perturbation, axis=0)\n",
    "        }\n",
    "        \n",
    "        # Print Results\n",
    "        if verbose:\n",
    "                print('\\n------------------------------------------------')\n",
    "                print(f'Attack Results: {model.name} (Clean vs Attack & Robustness Gap)')\n",
    "                print(f'Accuracy: {accuracy_clean:.4f} vs {accuracy_adv:.4f} & {robustness_gap_acc:.4f} ({100 * robustness_gap_acc/accuracy_clean:.4f}%)')\n",
    "                # print(f'Robustness Gap: {robustness_gap_acc:.4f}')\n",
    "                print(f'Specificity: {specificity_clean:.4f} vs {specificity_adv:.4f} & {robustness_gap_sp:.4f} ({100 * robustness_gap_sp/specificity_clean:.4f}%)')\n",
    "                # print(f'Robustness Gap: {robustness_gap_sp:.4f}')\n",
    "                print(f'Total Sample: {total_clean} vs {total_adv}')\n",
    "                print(f'False Positives: {fp_clean} vs {fp_adv}')\n",
    "                print(f'False Negatives: {fn_clean} vs {fn_adv}')\n",
    "                print('-----------')\n",
    "                print(f'Attack Success Rate: {attack_success_rate:.4f}')\n",
    "                \n",
    "                print('-----------')\n",
    "                print(f'Mean Bounds Utilisation: {avg_bound_utilization:.4f}')\n",
    "                print(f'Mean L∞ Perturbation: {linf_norm:.4f}')\n",
    "                print(f'Mean L2 Perturbation: {l2_norm:.4f}')\n",
    "                print(f'Mean L0 Perturbation: {l0_mean:.4f}')\n",
    "\n",
    "                print('Per-dimension mean absolute perturbation (first 10 dims):', np.round(dim_perturbation_stats['mean_abs_perturbation'][:10], 4))\n",
    "                print('Per-dimension max absolute perturbation (first 10 dims):',np.round(dim_perturbation_stats['max_abs_perturbation'][:10], 4))\n",
    "\n",
    "        results = {'model_name': model.name,\n",
    "                   'attack_success_rate': attack_success_rate\n",
    "                   }\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original query, labels and combine with predictions\n",
    "#\n",
    "\n",
    "def query_predictions(query_dataset_df, ref_ids, X, model):\n",
    "\n",
    "    # Map back on queries by ref-id\n",
    "    query_mapping = query_dataset_df[['ref-id', 'query', 'query-label-expert', 'query-is-non-medical']].set_index('ref-id').to_dict('index')\n",
    "\n",
    "    # Combine actual and predictions\n",
    "    # 'Non-Medical' (1) and 'Medical' (0)\n",
    "    predictions = model.predict(X)\n",
    "    predictions_bool = np.argmax(predictions, axis=1).astype(bool)\n",
    "\n",
    "    results = []\n",
    "    for i, ref_id in enumerate(ref_ids):\n",
    "        query_text = query_mapping[ref_id]['query']\n",
    "        query_label_expert = query_mapping[ref_id]['query-label-expert']\n",
    "        non_medical_actual = query_mapping[ref_id]['query-is-non-medical']\n",
    "        non_medical_predicted = predictions_bool[i]\n",
    "\n",
    "        results.append({\n",
    "            # 'ref_id': ref_id,\n",
    "            'query': query_text,\n",
    "            'query-label-expert': query_label_expert,\n",
    "            'query-is-non-medical-actual': non_medical_actual, \n",
    "            'query-is-non-medical-predicted': non_medical_predicted,\n",
    "            # 'pred_prob': predictions[i][0]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare Predictions to ground truth\n",
    "#\n",
    "\n",
    "def compare_predictions(query_predictions_df):\n",
    "    non_medical_predicted = (query_predictions_df['query-is-non-medical-predicted']).sum()\n",
    "    non_medical_incorrect = (~query_predictions_df['query-is-non-medical-actual'] & \n",
    "                             query_predictions_df['query-is-non-medical-predicted']).sum()\n",
    "    medical_predicted = len(query_predictions_df) - non_medical_predicted\n",
    "    medical_incorrect = (query_predictions_df['query-is-non-medical-actual'] & \n",
    "                         ~query_predictions_df['query-is-non-medical-predicted']).sum()\n",
    "\n",
    "    misclassified_df = query_predictions_df[query_predictions_df['query-is-non-medical-predicted'] != query_predictions_df['query-is-non-medical-actual']]\n",
    "\n",
    "    print(f'Predictions - Non-Medical: {non_medical_predicted} but {non_medical_incorrect} are incorrect')\n",
    "    print(f'Predictions - Medical: {medical_predicted} but {medical_incorrect} are incorrect')\n",
    "    print('Example Misclassifications')\n",
    "    display(misclassified_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40937b23",
   "metadata": {},
   "source": [
    "***\n",
    "# Run - Compare Simple PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Common Training Parameters & Run Setup\n",
    "#\n",
    "batch_size = 128\n",
    "\n",
    "local_project_folder = Path.cwd().parent\n",
    "data_folder, run_results_folder = run_setup(local_project_folder, 'Simple_PGD_1')\n",
    "\n",
    "#\n",
    "# Get source medical data & add embeddings\n",
    "#\n",
    "queries_raw_df = get_query_data(data_folder, run_results_folder)\n",
    "queries_raw_df.to_pickle(run_results_folder.joinpath('queries_data_raw.pkl'))\n",
    "\n",
    "queries_clean_df = queries_raw_df.copy()\n",
    "queries_clean_df['query-embedding'] = list(create_embeddings(queries_raw_df['query'].values))\n",
    "queries_clean_df.to_pickle(run_results_folder.joinpath('queries_clean_df.pkl'))\n",
    "print(f\"Query Embeddings Shape: {queries_clean_df.shape}\")\n",
    "\n",
    "#\n",
    "# Initial visualisation of text & embeddings\n",
    "#\n",
    "inspect_query_df(queries_clean_df)\n",
    "# TODO: Visualisation, clustering, t-SNE, UMAP etc ....\n",
    "\n",
    "#\n",
    "# Prepare Training Data\n",
    "#\n",
    "# Split data & fit PCA pipe to X_train and apply to X_val, X_test\n",
    "# Assumes no K-Fold validation, and the need to split before PCA\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test = prep_data_splits(queries_clean_df)\n",
    "pca_pipe, X_train_pca, X_val_pca, X_test_pca = pca_create(X_train, X_val, X_test)\n",
    "\n",
    "# Create tf datasets\n",
    "train_dataset_tf, val_dataset_tf = prep_tf_datasets(X_train_pca, X_val_pca, y_train, y_val, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc932486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build & Train Base Models\n",
    "#\n",
    "\n",
    "# All models training parameters\n",
    "embedding_dimensions = X_train_pca.shape[1]\n",
    "epochs = 2 # 30\n",
    "alpha = 0.7\n",
    "\n",
    "# PGD / Hyperrectangle training parameters / PGD Atack parameters\n",
    "beta = 0.3\n",
    "pgd_steps = 2 #10           # TODO: What step size to set?\n",
    "gamma_multiplier = 750     # TODO: Higher Gamma increases strength, eg 750 high FPs\n",
    "hyper_type = 'min_max'     # TODO: Expand to different types of hyperrectangle calc\n",
    "\n",
    "# Build & Train Base Model - Clean Data\n",
    "base_model = build_compile_model(embedding_dimensions, 'classifer_clean')\n",
    "base_model.fit(train_dataset_tf, epochs=epochs, validation_data=val_dataset_tf)\n",
    "inspect_model(base_model)\n",
    "base_model.save(run_results_folder.joinpath('base_model.keras'))\n",
    "\n",
    "# Build & Train Tuned Model - Clean Data\n",
    "base_model_tuned_clean = get_tuned_model(embedding_dimensions, epochs, 'classifier_tuned_clean', run_results_folder, train_dataset_tf, val_dataset_tf)\n",
    "base_model_tuned_clean = custom_model_train(base_model_tuned_clean, epochs, train_dataset_tf, val_dataset_tf, alpha)\n",
    "inspect_model(base_model_tuned_clean)\n",
    "base_model_tuned_clean.save(run_results_folder.joinpath('base_model_tuned_clean.keras'))\n",
    "\n",
    "# Build & Train Tuned Model - Adversarial\n",
    "base_model_tuned_adv = get_tuned_model(embedding_dimensions, epochs, 'classifier_tuned_adv', run_results_folder, train_dataset_tf, val_dataset_tf)\n",
    "base_model_tuned_adv = custom_model_train(base_model_tuned_adv, epochs, train_dataset_tf, val_dataset_tf, alpha,\n",
    "                                          beta, pgd_steps, gamma_multiplier)\n",
    "inspect_model(base_model_tuned_adv)\n",
    "base_model_tuned_adv.save(run_results_folder.joinpath('base_model_tuned_adv.keras'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcca1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate Adversarial Attacks\n",
    "#\n",
    "\n",
    "hyper_type = 'min_max' \n",
    "_, X_adv, y_adv, hyper_bounds = generate_PGD_dataset(X_test_pca, y_test, base_model_tuned_clean, pgd_steps, gamma_multiplier, batch_size, hyper_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Evaluate Models\n",
    "#\n",
    "\n",
    "models_comparisons_df = pd.DataFrame()\n",
    "\n",
    "# Models predicting on clean data + Sentence Text \n",
    "#\n",
    "model_results = evaluate_model(base_model, X_test_pca, y_test, verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "query_predictions_df = query_predictions(queries_clean_df, ids_test, X_test_pca, base_model)\n",
    "compare_predictions(query_predictions_df)\n",
    "\n",
    "model_results = evaluate_model(base_model_tuned_clean, X_test_pca, y_test, verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "query_predictions_df = query_predictions(queries_clean_df, ids_test, X_test_pca, base_model_tuned_clean)\n",
    "compare_predictions(query_predictions_df)\n",
    "\n",
    "# Models Predicting on Adversarial data\n",
    "#\n",
    "model_results = evaluate_model(base_model, X_adv, y_adv, data_source='Adv_PGD', verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "\n",
    "model_results = evaluate_model(base_model_tuned_clean, X_adv, y_adv, data_source='Adv_PGD', verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "\n",
    "# Adversarial Trained Model on Clean & Adversarial Data\n",
    "#\n",
    "model_results = evaluate_model(base_model_tuned_adv, X_test_pca, y_test, verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "\n",
    "model_results = evaluate_model(base_model_tuned_adv, X_adv, y_adv, data_source='Adv_PGD', verbose=False)\n",
    "models_comparisons_df = pd.concat([models_comparisons_df, pd.DataFrame([model_results])], ignore_index=True)\n",
    "\n",
    "# Show Models Comparison & Save\n",
    "print('Model Results Comparisons')\n",
    "display(models_comparisons_df)\n",
    "models_comparisons_df.to_pickle(run_results_folder.joinpath('models_comparisons.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e351660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate Model Robustness\n",
    "#\n",
    "\n",
    "# Adversarial Attack Details\n",
    "# - Semantic Preservation - cos similarity difference\n",
    "# - Perturbation magnitude - L2 etc ??\n",
    "# - Hyperrectangle visualisation\n",
    "# - key parameters and impact\n",
    "\n",
    "# Robustness Metrics\n",
    "# \n",
    "\n",
    "# Base Model\n",
    "results_base = evaluate_model_attack(base_model_tuned_clean, X_test_pca, y_test, X_adv, y_adv, hyper_bounds, verbose=True)\n",
    "\n",
    "# Model PGD Trained\n",
    "results_adv = evaluate_model_attack(base_model_tuned_adv, X_test_pca, y_test, X_adv, y_adv, hyper_bounds, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23b50650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperrectangles Creation - min_max shape: (301, 30, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 13:44:47.603618: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [301]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "# Attack with reduced sample\n",
    "#\n",
    "\n",
    "# Extract only the positive X, y, ie non-medical\n",
    "mask_test_pos = (y_test == 1)\n",
    "X_2 = X_test_pca[mask_test_pos]\n",
    "y_2 = y_test[mask_test_pos]\n",
    "\n",
    "hyper_type = 'min_max' \n",
    "_, X_adv_2, y_adv_2, hyper_bounds_2 = generate_PGD_dataset(X_2, y_2, base_model_tuned_clean, pgd_steps, gamma_multiplier, batch_size, hyper_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65ee5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 295us/step\n",
      "10/10 [==============================] - 0s 369us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/08zshp4n3z1_glk6l5vgyqz40000gn/T/ipykernel_77639/664814345.py:133: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity_adv = tn_adv / (tn_adv + fp_adv)\n",
      "/var/folders/_3/08zshp4n3z1_glk6l5vgyqz40000gn/T/ipykernel_77639/664814345.py:137: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  attack_success_rate = np.mean(y_pred_clean != y_pred_adv)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (301,30) (584,30) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Base Model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results_base_2 = \u001b[43mevaluate_model_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_tuned_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_adv_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_adv_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Model PGD Trained\u001b[39;00m\n\u001b[32m      5\u001b[39m results_adv_2 = evaluate_model_attack(base_model_tuned_adv, X_test_pca, y_test, X_adv_2, y_adv_2, hyper_bounds, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mevaluate_model_attack\u001b[39m\u001b[34m(model, X_clean, y_clean, X_adv, y_adv, bounds, verbose)\u001b[39m\n\u001b[32m    139\u001b[39m robustness_gap_sp = specificity_clean - specificity_adv\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Perturbation statistics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m perturbation = \u001b[43mX_adv\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_clean\u001b[49m\n\u001b[32m    143\u001b[39m lower_bounds, upper_bounds = bounds\n\u001b[32m    144\u001b[39m perturbation_ranges = upper_bounds - lower_bounds\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (301,30) (584,30) "
     ]
    }
   ],
   "source": [
    "\n",
    "# Base Model\n",
    "results_base_2 = evaluate_model_attack(base_model_tuned_clean, X_test_pca, y_test, X_adv_2, y_adv_2, hyper_bounds, verbose=True)\n",
    "\n",
    "# Model PGD Trained\n",
    "results_adv_2 = evaluate_model_attack(base_model_tuned_adv, X_test_pca, y_test, X_adv_2, y_adv_2, hyper_bounds, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e031cd",
   "metadata": {},
   "source": [
    "***\n",
    "# Run - Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Run\n",
    "#\n",
    "local_project_folder = Path.cwd().parent\n",
    "data_folder, run_results_folder = run_setup(local_project_folder, 'Test_1')\n",
    "\n",
    "\n",
    "# Get source medical data\n",
    "#\n",
    "queries_clean_df = get_query_data(data_folder, run_results_folder)\n",
    "queries_clean_df.to_pickle(run_results_folder.joinpath('queries_data_raw.pkl'))\n",
    "\n",
    "# Create and add perturbations\n",
    "#\n",
    "queries_all_df = create_perturbations(queries_clean_df)\n",
    "\n",
    "# Create and add embeddings\n",
    "# \n",
    "queries_all_df['query-embedding'] = list(create_embeddings(queries_all_df['query'].values))\n",
    "queries_all_df.to_pickle(run_results_folder.joinpath('queries_all_df.pkl'))\n",
    "print(f\"Query Embeddings Shape: {queries_all_df.shape}\")\n",
    "\n",
    "# queries_perturbations_df['query-embedding'] = list(create_embeddings(queries_perturbations_df['query'].values))\n",
    "# queries_perturbations_df.to_pickle(run_results_folder.joinpath('queries_perturbations_df.pkl'))\n",
    "# print(f\"Query Embeddings Perturbations shape: {queries_perturbations_df.shape}\")\n",
    "\n",
    "# Initial visualisation of text & embeddings\n",
    "#\n",
    "inspect_query_df(queries_clean_df)\n",
    "inspect_query_df(queries_all_df)\n",
    "# TODO: Visualisation, clustering etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X,y with Train, Val, Test \n",
    "# !! Assumes no K-Fold validation, and need to split before PCA\n",
    "#\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test = prep_data_splits(queries_all_df)\n",
    "\n",
    "# Fit PCA Pipe to X_train and apply on X\n",
    "train_PCA, X_train_pca, X_val_pca, X_test_pca = pca_create(X_train, X_val, X_test)\n",
    "\n",
    "# Create tf datasets\n",
    "train_dataset_tf, val_dataset_tf = prep_tf_datasets(X_train_pca, X_val_pca, y_train, y_val)\n",
    "\n",
    "# Dimensions\n",
    "embedding_dimensions = X_train_pca.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ece9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subspaces Generate\n",
    "# Create Hyperrectangles from Embedded Sentences, Clean & Perturbed\n",
    "#\n",
    "\n",
    "# TODO: Using ALL X_train whereas ANTONIO uses just positive\n",
    "# hyperrectangles shape (N, D, 2), 2 is upper, lower bounds\n",
    "hyperrectangles_X_train = generate_hyperrectangles('min_max', X_train_pca)\n",
    "np.savez_compressed(run_results_folder.joinpath(\"hyperrectangles\"), hyperrectangles_X_train)\n",
    "\n",
    "# Inspect, visualise hyperrectangles\n",
    "# TODO: How best to do this?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60945ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier NN Training\n",
    "#\n",
    "# Base NN\n",
    "# Perturbations\n",
    "# PGD\n",
    "\n",
    "\n",
    "# Verification\n",
    "#\n",
    "# TODO: TBD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43086258",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_perturbations(queries_clean_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDT_DAIR_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
